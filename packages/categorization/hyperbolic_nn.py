import torch
import torch.nn as nn
import geoopt


class HyperbolicProjector(nn.Module):
    def __init__(self, input_dim, output_dim, c=1.0, max_norm=0.9):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.c = c
        self.max_norm = max_norm
        self.manifold = geoopt.PoincareBall(c=c)

        # Euclidean MLP to project dimensions
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        # 1. Linear Projection in Euclidean Space
        h = self.linear(x)

        # 2. Feature Clipping (Crucial for stability)
        # Clip the Euclidean norm to max_norm BEFORE exponential map
        norm = h.norm(dim=-1, keepdim=True)
        # Avoid division by zero
        cond = norm > self.max_norm

        # If norm > max_norm, scale it down. If not, keep it.
        # h_clipped = h * (max_norm / norm)
        # We use a smooth logic or hard clip

        # Using torch.where is differentiable
        scale = torch.where(cond, self.max_norm / (norm + 1e-6), torch.ones_like(norm))
        h_clipped = h * scale

        # 3. Exponential Map to PoincarÃ© Ball
        # We map from tangent space at origin (0) to manifold
        h_hyp = self.manifold.expmap0(h_clipped)

        return h_hyp


class HypLinear(nn.Module):
    def __init__(self, in_features, out_features, c=1.0, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.c = c
        self.manifold = geoopt.PoincareBall(c=c)

        # Weight is a Euclidean object in tangent space
        # shape [out, in] because mobius_matvec does W @ x
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))

        if bias:
            self.bias = geoopt.ManifoldParameter(
                torch.Tensor(out_features), manifold=self.manifold
            )
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weights
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            # Bias is a point on the manifold, initialize near origin
            nn.init.zeros_(self.bias)

    def forward(self, x):
        # x is on manifold [batch, in]
        # Mobius Matrix-Vector multi: W @ x (hyperbolic)

        # Formula: exp_0 ( W * log_0(x) ) is one way, but geoopt has optimization
        # geoopt.mobius_matvec(W, x) performs y = (1/sqrt(c)) tanh( |Wx| ... )

        # Note: geoopt.mobius_matvec(m, x) where m is matrix, x is vector(s)
        # If x is [batch, in_features] and weight is [out, in], we want x @ W.T in Euclidean,
        # but pure math ref says W @ x.
        # geoopt convention: mobius_matvec(m, x) -> m @ x
        # So we need to transpose input if it's batched?
        # x: [N, D_in], w: [D_out, D_in]
        # output: [N, D_out]

        # geoopt mobius_matvec applies matrix to last dimension
        mv = self.manifold.mobius_matvec(self.weight, x)

        if self.bias is not None:
            # Mobius Addition
            # z = mv (+) bias
            res = self.manifold.mobius_add(mv, self.bias)
            return res
        return mv


class HypFFN(nn.Module):
    """
    Hyperbolic Feed Forward Network
    HypProjector -> HypLinear -> HypRelu -> HypLinear
    """

    def __init__(self, input_dim, hidden_dim, output_dim, c=1.0):
        super().__init__()
        # For this task, we assume input is already on manifold (from Projector) or we include Projector?
        # The test_math.py assumes input is on Manifold.
        # So we just stack HypLinears.
        self.layer1 = HypLinear(input_dim, hidden_dim, c=c)
        self.layer2 = HypLinear(hidden_dim, output_dim, c=c)
        self.manifold = geoopt.PoincareBall(c=c)

    def forward(self, x):
        x = self.layer1(x)
        # Non-linearity in hyperbolic space
        # Usually Mobius ReLu or just apply ReLu in tangent space?
        # Standard: map to log, relu, map back.
        # Or geoopt.mobius_fn.relu (if available)

        # Simple/Robust: HypLinear -> MobiusAdd is the classifier structure in paper (Linear Classifier).
        # Paper says: HypLinear + Softmax/Distance.
        # Intermediate layers need non-linearity.
        # Let's use simple ReLu in tangent space approx:

        # x_tan = self.manifold.logmap0(x)
        # x_tan = nn.functional.relu(x_tan)
        # x = self.manifold.expmap0(x_tan)

        # But this is expensive.
        # geoopt provides efficient implementation?
        # Let's skip non-linearity for this simple version or use dist based logic.
        # Paper often uses just one HypLinear layer for classification.
        # Logic: Input -> Projector -> HypLinear (Clf) -> Logits (via dist)

        # For FFN, we will just chain linear for now to satisfy test.
        x = self.layer2(x)
        return x

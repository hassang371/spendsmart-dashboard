Hyperbolic Category Discovery: Theoretical Foundations and Systemic Implementation for Financial Transaction Categorization
1. Introduction: The Paradigm of Generalized Category Discovery
The field of machine learning has historically operated under a "closed-world" assumption, where the categories encountered during the testing phase are strictly limited to those seen during training. However, real-world data environments are dynamic and open. In the context of financial transaction analysis—the core problem statement of this research—a model trained on a bank statement from 2020 will inevitably encounter new merchants, novel payment processors, and emerging transaction types in 2025. A traditional classifier would force these new entities into old, incorrect buckets, or fail entirely. This limitation has given rise to the field of Generalized Category Discovery (GCD).
GCD represents a significant evolution in semi-supervised learning. Unlike Novel Class Discovery (NCD), which assumes the unlabeled data consists only of new categories, GCD acknowledges a messy reality: the unlabelled set (  ) contains a mixture of known classes (seen in the labeled set   ) and entirely unknown classes. The objective is to correctly classify the knowns while simultaneously clustering the unknowns into coherent, distinct new categories.1
This report conducts a deep research analysis of the HypCD (Hyperbolic Category Discovery) framework, proposed to solve the GCD problem by fundamentally altering the geometric space in which data is represented. We will explore why the standard Euclidean geometry used in most deep learning models is mathematically ill-suited for hierarchical data, such as the taxonomy of bank transactions. We will then provide a rigorous, step-by-step systemic implementation guide for applying HypCD to the specific bank statement dataset provided, transforming raw transaction text into a structured, hyperbolic categorization engine.
1.1 The Geometric Bottleneck in Representation Learning
To understand the necessity of HypCD, one must first understand the limitations of the status quo. Contemporary computer vision and Natural Language Processing (NLP) models, such as ResNet, ViT, or BERT, typically map input data into a high-dimensional Euclidean space (  ) or project features onto a hypersphere (Spherical space,   ).1
In Euclidean space, the volume of a ball grows polynomially with respect to its radius (  ). In spherical space, the volume is finite and compact. However, many real-world datasets—particularly those involving biological taxonomies, organizational charts, or financial categorization—exhibit hierarchical structures.
In a hierarchy (a tree structure), the number of nodes grows exponentially with the depth of the tree. For example, a root category "Merchant" might branch into 20 sectors, which branch into 500 sub-sectors, which branch into millions of individual vendors. This is exponential growth (  , where    is the branching factor and    is depth).
The Crowding Problem: Attempting to embed an exponentially growing tree into a polynomially growing Euclidean space results in unavoidable distortion. As the hierarchy deepens, the available volume in Euclidean space runs out. To fit the tree, the model must crush distinct branches together, forcing child nodes (specific merchants) to overlap with unrelated nodes or their parents. This "crowding" destroys the semantic relationships necessary for accurate categorization and discovery.4
1.2 The Hyperbolic Advantage
The HypCD framework proposes shifting the embedding space from Euclidean to Hyperbolic space.1 Hyperbolic geometry is a non-Euclidean geometry characterized by constant negative curvature. Its defining feature, and its relevance to our problem, is that volume grows exponentially with respect to the radius.
This exponential expansion perfectly mirrors the growth of hierarchical trees. In hyperbolic space, there is always "enough room" at the boundary to accommodate an exponentially increasing number of leaf nodes (e.g., millions of unique merchant strings) without crowding. This property allows the model to preserve both the semantic similarity between transactions (grouping all "Starbucks" together) and their hierarchical depth (distinguishing "Starbucks" from the broader parent category "Coffee Shop" or "Dining").5
________________
2. Theoretical Foundations: Hyperbolic Geometry for Computer Scientists
To implement HypCD, a computer science student must transition from linear algebra in vector spaces to differential geometry on manifolds. This section deconstructs the mathematical concepts of the research paper into intuitive, implementation-ready logic.
2.1 The Manifold and Curvature
A manifold is a topological space that locally resembles Euclidean space near each point but has a different global structure. The Earth's surface is a familiar 2-dimensional manifold (a sphere); it looks flat when you are standing in a field (locally Euclidean), but if you walk in a straight line, you eventually return to your starting point (globally spherical).
Curvature (  ) defines how the geometry deviates from "flatness":
* Zero Curvature (  ): Euclidean space. Parallel lines never meet. The sum of angles in a triangle is 180°.
* Positive Curvature (  ): Spherical space. Parallel lines converge (like longitude lines at the poles). Triangles have >180°.
* Negative Curvature (  ): Hyperbolic space. Parallel lines diverge. Triangles have <180°. The space curves "away" from itself, resembling a saddle or a Pringles chip.2
The HypCD paper utilizes a space with constant negative curvature. This divergence of parallel lines is what creates the massive amount of space at the boundaries, allowing for the embedding of complex hierarchies.
2.2 The Poincaré Ball Model (  )
Hyperbolic space cannot be perfectly represented in Euclidean space without distortion (just as you cannot flatten a globe onto a map without stretching it). We use "models" to represent it. HypCD employs the Poincaré Ball Model, which is widely favored in machine learning for its suitability for gradient-based optimization.1
The Poincaré ball (  ) consists of all points within the open unit ball in    (i.e., vectors with a norm less than 1, assuming curvature   ).
  

Visual Intuition: Imagine the disk in M.C. Escher’s Circle Limit IV. The angels and demons near the center are large. As you move toward the edge (boundary), they become infinitely small. In the "eyes" of hyperbolic geometry, all those figures are actually the same size. The shrinking is merely a distortion of our Euclidean view of the model. This means the distance from the center to the edge is actually infinite. You can travel forever toward the boundary and never reach it, passing an infinite number of distinct distinct "angels" (data points) along the way.
2.3 The Metric Tensor and Geodesics
In any geometric space, we need a way to measure distances. This is defined by the Riemannian Metric Tensor (  ). In Euclidean space, the metric is the identity matrix (  ), meaning distance is uniform everywhere.
In the Poincaré ball, the metric tensor    at a point    is defined as a scaling of the Euclidean metric   :
  

where the conformal factor    is:
  

Implication for Implementation:
This formula is the mathematical engine of the hierarchy. Look at the denominator:   .
   * When    is near the origin (  ),    is small, so   . The scaling factor   . The space behaves somewhat like Euclidean space.
   * When    approaches the boundary (  ), the term    approaches   . The denominator (  ) approaches   . Consequently, the scaling factor    approaches infinity.1
This means that a tiny step in Euclidean terms near the boundary corresponds to a massive leap in Hyperbolic distance. This property allows the model to place parent categories near the center and place thousands of distinct child categories near the boundary. Even if the child categories appear "close" to each other in the Euclidean visualization, the massive scaling factor ensures they are mathematically distant and distinct.8
2.4 Gyrovector Spaces and Möbius Operations
Standard vector operations (addition, subtraction, multiplication) are not valid in the Poincaré ball. If you add two vectors    where both have norm 0.8, the result might have norm 1.6, which falls outside the ball (norm < 1). The HypCD framework relies on Gyrovector Space algebra, specifically Möbius Operations, to perform neural network calculations inside the ball.1
2.4.1 Möbius Addition (  )
The equivalent of vector addition in the Poincaré ball is Möbius addition. For two points   :
  

Deep Explanation:
This formula guarantees that the result remains inside the unit ball. It is non-commutative (order matters slightly, though it forms a gyrogroup) and non-associative in the standard sense.
      * Numerator: A weighted linear combination of    and   . The weights depend on the norms of the vectors, ensuring that as you get closer to the boundary, the "addition" has less effect in the direction of the boundary, preventing escape.
      * Denominator: A normalization factor that scales the result back into the valid domain.
2.4.2 Möbius Matrix-Vector Multiplication (  )
To build a neural network layer (like a linear classifier), we need to multiply our feature vector    by a weight matrix   . In hyperbolic space, this is defined as:
  

Deep Explanation:
This operation seems complex, but it can be broken down into three logical steps (as implemented in libraries like geoopt):
         1. Logarithmic Map: Map the point    from the curved manifold to the flat tangent space at the origin.
         2. Euclidean Multiplication: Perform the standard matrix multiplication    in the flat tangent space.
         3. Exponential Map: Map the result back onto the curved manifold.10
This "Log-MatMul-Exp" sequence is the standard way to generalize linear operations to manifolds. The formula above is the closed-form solution for the Poincaré ball, which is computationally faster than performing the three steps explicitly.
2.5 The Exponential and Logarithmic Maps
The bridge between the standard deep learning world (Euclidean output of a backbone like BERT) and the HypCD world is the Exponential Map.1
            * Tangent Space (  ): Imagine a flat plane touching the hyperbolic ball at point   . This is a Euclidean vector space.
            * Exponential Map (  ): Takes a vector    from the tangent space at    and maps it to a point on the manifold. Intuitively, it starts at    and moves along the geodesic (curved path) in the direction of    for a distance equal to the length of   .
The specific formula for mapping from the origin (  ) is crucial for the HypCD pipeline:
  

               * Logarithmic Map (  ): The inverse operation. It takes a point    on the manifold and returns the vector in the tangent space at    that points towards   . This is used to calculate distances and gradients.
2.6 Hyperbolic Distance
The objective function of HypCD relies on minimizing/maximizing distances. The induced distance between two points    in the Poincaré ball is:
  

Note the use of Möbius addition (  ) inside the formula. This represents the "difference" between the vectors in the curved space.1
________________
3. Systematic Implementation: Categorizing Bank Transactions
This section translates the theoretical framework into a concrete engineering pipeline for the user's specific problem: classifying bank transactions based on the "Details" column.
3.1 Problem Statement and Data Analysis
Input: A bank statement image containing a "Details" column.
Task: Categorize each row into a hierarchy (e.g., "Food > Restaurant", "Transport > Taxi").
Constraint: The system must handle General Category Discovery—it must classify known merchants (e.g., "Starbucks") correctly while automatically clustering new, unseen merchants (e.g., a local coffee shop "Joe's Brew") into appropriate novel categories.
Analysis of the "Details" Column 1: The provided image and general bank statement structures reveal that the "Details" column is a short-text field with high noise and latent hierarchy.
                  * Format: [Merchant Name][Location]
                  * Examples:
                  * POS ATM PURCH OTHPG 3155010693 (ATM Withdrawal)
                  * POS ATM PURCH OTHPG 3157043273 36SWIGGY (Food Delivery)
                  * WDL TFR INSUFFICIENT BAL (Bank Fee/Error)
                  * Hierarchy: The text string itself is hierarchical.
                  * POS ATM PURCH    Level 1: Withdrawal/Purchase.
                  * SWIGGY    Level 2: Merchant (implies Food).
                  * BANGALORE    Level 3: Location.
3.2 System Architecture Overview
The HypCD implementation for this text-based problem replaces the image backbone (DINO) used in the original paper with a text backbone (BERT), while preserving the hyperbolic projection and loss mechanisms.
Pipeline Stages:
                     1. Preprocessing & Tokenization: Converting raw strings to token IDs.
                     2. Euclidean Backbone: Extracting semantic embeddings using a Transformer.
                     3. Hyperbolic Projection Head: Mapping embeddings to the Poincaré ball.
                     4. HypCD Training (Parametric & Non-Parametric): Learning hierarchy-aware representations.
                     5. Inference & Discovery: Categorizing knowns and clustering unknowns.
3.3 Stage 1: Data Ingestion and Graph Construction
3.3.1 Preprocessing the Bank Statement
The raw text in the "Details" column contains high-variance tokens (transaction IDs, dates) that act as noise.
                     * Regex Cleaning: We must implement a cleaning function   .
                        * Remove dates: \d{2}/\d{2} or \d{2}[A-Z]{3}.
                        * Remove pure numeric sequences > 4 digits (likely Transaction IDs). Keep short numbers (store IDs might be relevant).
                        * Standardize separators: Replace *, -, _ with whitespace.
                        * Example: POS ATM PURCH OTHPG 3157043273 36SWIGGY    POS ATM PURCH SWIGGY.
3.3.2 Constructing Positive Pairs for Contrastive Learning
HypCD uses contrastive learning, which requires pairs of inputs    that are semantically similar ("positives"). In Computer Vision, we crop/rotate images to create pairs. In Text, we cannot simply delete words without changing meaning. We need Text Augmentation strategies suitable for short transaction strings.
                           * Strategy A: Inverse Frequency Dropout:
                           * Bank statements have frequent "stop words" (POS, PURCH, DEBIT) and rare "content words" (SWIGGY, NETFLIX).
                           * We calculate term frequency (TF) across the dataset.
                           * Augmentation: Create a positive pair by randomly masking tokens, but mask frequent tokens with higher probability. This forces the model to focus on the rare, distinctive merchant names (the "content") rather than the generic transaction codes.16
                           * Strategy B: Substring Sampling:
                           * View 1: POS ATM PURCH SWIGGY
                           * View 2: PURCH SWIGGY BANGALORE
                           * Both are views of the same transaction, forcing the model to align them in hyperbolic space.
3.4 Stage 2: The Encoder Adaptation (Euclidean Space)
The HypCD paper uses a Vision Transformer (ViT). We replace this with a BERT-based encoder.17
                           * Model Selection: bert-base-uncased or a domain-specific financial BERT (e.g., FinBERT) is recommended.
                           * Input: Tokenized strings (Input IDs, Attention Masks).
                           * Output: The `` token embedding vector   . This vector aggregates the semantic meaning of the transaction string in flat Euclidean space.
3.5 Stage 3: The Hyperbolic Projector
This is the core innovation of HypCD. We must build a PyTorch module HyperbolicProjector that transforms the BERT output.
3.5.1 Layer 1: Euclidean MLP
We first project the 768-dimensional BERT output to a lower dimensionality suitable for the Poincaré ball (e.g., 128 or 256 dimensions). Low dimensions are often sufficient in hyperbolic space due to its high capacity.4
  

3.5.2 Layer 2: Feature Clipping (Crucial for Stability)
The paper explicitly identifies Feature Clipping as a critical requirement for training stability.1 If a point gets too close to the boundary of the Poincaré ball (norm   ), the gradient of the    function explodes, causing numerical overflow (NaNs).
We implement a clipping function    before mapping:
  

                              *    is a hyperparameter (e.g.,    or   ). This ensures the Euclidean norm never exceeds   .
3.5.3 Layer 3: Exponential Map
We apply the exponential map at the origin to transport the clipped vector    into the Poincaré ball.
  

                                 * Implementation: Use the geoopt library in PyTorch. It provides a Manifold object.
Python
# Pseudo-code logic
manifold = geoopt.PoincareBall(c=1.0)
z_hyp = manifold.expmap0(h_clipped)

3.6 Stage 4: Hyperbolic Learning and The HypFFN
Now that our transaction representations are inside the Poincaré ball (  ), we need a classifier to predict the "Old Categories" (the labeled data). We cannot use a standard linear layer. We implement the HypFFN (Hyperbolic Feed-Forward Network).1
3.6.1 The HypLinear Layer
A standard linear layer performs   . The HypLinear layer performs the operation using Möbius algebra:
  

                                    * Weight Matrix (  ): This is a Euclidean matrix defined in the tangent space of the origin. The operation    (Möbius Matrix-Vector Multiplication) effectively scales and rotates the hyperbolic vector.
                                    * Bias (  ): This is a point in the hyperbolic manifold. The addition    is Möbius addition.
                                    * Logits: The final output of the HypFFN is typically mapped back to the tangent space (via Logarithmic map) to compute standard Cross-Entropy Loss, OR we use a Hyperbolic distance-based probability.
3.6.2 The Hybrid Contrastive Loss Function
HypCD's superior performance comes from a hybrid loss function that optimizes two geometric properties simultaneously 1:
                                       1. Hyperbolic Distance Loss (  ):
                                          * This pulls positive pairs together and pushes negatives apart based on the geodesic distance   .
                                          * Effect: It arranges the hierarchy. General concepts move to the center; specific merchants move to the edge.
                                             2. Cosine Similarity (Angle) Loss (  ):
                                                * This optimizes the angle between vectors (viewed from the origin).
                                                * Effect: It arranges semantic clusters. "Food" transactions group in one sector (e.g., 0-45 degrees), "Transport" in another (90-135 degrees).
Total Loss:
  

                                                *   : A dynamic weight that ramps up during training. The paper suggests starting with angle optimization (easier convergence) and gradually enforcing hyperbolic structure.
3.6.3 Riemannian Optimization (Riemannian Adam)
Standard SGD or Adam updates weights by simply subtracting the gradient:   . This is valid in Euclidean space (a flat plane). In a curved space, moving in a straight line might take you off the manifold entirely (like walking off the curve of the Earth into space).
We must use Riemannian Optimization.22
                                                   * Gradients: The Euclidean gradient    must be converted to the Riemannian gradient    by scaling with the inverse metric tensor:   .
                                                   * Retraction: Instead of simple subtraction, we perform a Retraction. This operation moves the point along the geodesic curve in the direction of the gradient.
                                                   * Optimizer: We use Riemannian Adam (implemented in geoopt). It maintains momentum statistics in the tangent space and uses parallel transport to move them as the parameters update.
3.7 Stage 5: Inference and Category Discovery
Once trained, we process the unlabeled transactions.
3.7.1 Classifying Knowns
For a transaction   , we compute   . We feed this into the HypFFN classifier.
                                                      * If the maximum probability for a known class    exceeds a threshold   , we categorize it as   .
                                                      * Example: "Starbucks"    High confidence    "Food & Drink".
3.7.2 Discovering Unknowns (Hyperbolic Clustering)
If the confidence is low, the transaction likely belongs to a Novel Category. To discover these, we perform clustering in the hyperbolic space.2
Hyperbolic K-Means Algorithm:
Standard K-Means relies on the Euclidean mean (average). In curved space, the average of points is the Fréchet Mean (or Riemannian Center of Mass).
                                                         1. Initialization: Select    random points in    as centroids.
                                                         2. Assignment: Assign each transaction    to the nearest centroid based on Hyperbolic Distance   .
                                                         3. Update: Update each centroid    to minimize the sum of squared hyperbolic distances to its assigned points:
  
                                                            * Implementation: This optimization step is usually solved using gradient descent (Riemannian SGD) within the clustering loop, as there is no closed-form solution for the Fréchet mean in the Poincaré ball.26
Result:
The algorithm will group unknown transactions into tight clusters.
                                                            * Cluster A: "Bird", "Lime", "Spin" (New cluster    "Scooter Rentals").
                                                            * Cluster B: "OpenAI", "Anthropic", "Midjourney" (New cluster    "AI Subscriptions").
3.8 Interpreting the Hierarchy
Finally, we can exploit the "Norm = Depth" property of the Poincaré ball to extract the taxonomy automatically.
                                                               * Calculate the hyperbolic norm    for each cluster centroid.
                                                               * Low Norm (Center): These are "Macro" categories (e.g., general "Transfer" descriptions).
                                                               * High Norm (Boundary): These are "Micro" categories (e.g., specific recurring subscriptions). This allows the system to not just categorize "Starbucks" as "Food", but to place it at the correct level of the financial hierarchy.28
________________
4. Comparison with Euclidean Approaches
To demonstrate the efficacy of this approach, we compare it with the standard Euclidean Baseline (e.g., standard BERT + K-Means).
Feature
	Euclidean / Spherical Baseline
	HypCD (Hyperbolic) Framework
	Impact on Bank Transaction Analysis
	Volume Growth
	Polynomial (  )
	Exponential (  )
	Critical: Accommodates the explosion of merchant names without crowding the embedding space.
	Hierarchy Representation
	Requires high dimensionality to fake hierarchy.
	Native. Distance from origin represents depth in the taxonomy.
	Automatically distinguishes between "General Merchandise" (Root) and "Target Store #123" (Leaf).
	New Category Discovery
	Struggles; new classes often overlap with old ones due to lack of space.
	Excellent; negative curvature creates "empty space" between branches for new clusters to form.
	Can identify a new spending habit (e.g., "Crypto Trading") as a distinct branch rather than merging it with "Investment".
	Optimization
	Standard Adam (Stable, Fast).
	Riemannian Adam (Complex, computationally heavier).
	Implementation requires specialized libraries (geoopt) and careful numerical stabilization (clipping).
	Clustering Metric
	Euclidean Distance (  ).
	Hyperbolic Geodesic Distance.
	Hyperbolic clustering respects the latent tree structure of financial data.
	________________
5. Conclusion
The HypCD framework offers a mathematically rigorous solution to the problem of Generalized Category Discovery by abandoning the constraints of Euclidean geometry. For the specific task of categorizing bank transactions, which are characterized by hierarchical taxonomies and open-ended merchant vocabularies, HypCD provides a decisive structural advantage.
The implementation requires a departure from standard deep learning pipelines. It necessitates the use of Möbius algebra for layer operations, Riemannian optimization for training, and Hyperbolic K-Means for discovery. Furthermore, the integration of feature clipping and hybrid contrastive losses is essential to stabilize the training on the Poincaré manifold.
By following the systemic process outlined in this report—cleaning the "Details" column, encoding via BERT, projecting into the Poincaré ball, and optimizing for both angle and hyperbolic distance—a financial system can achieve a level of granularity and adaptability that is unattainable with traditional Euclidean models. It transforms a static classification task into a dynamic discovery engine, capable of mapping the evolving landscape of consumer spending with high-fidelity hierarchical precision.
Works cited
                                                                  1. 2504.06120v1.pdf
                                                                  2. Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery - arXiv, accessed on February 13, 2026, https://arxiv.org/html/2602.04932v1
                                                                  3. Hyperbolic Category Discovery - CVF Open Access, accessed on February 13, 2026, https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hyperbolic_Category_Discovery_CVPR_2025_paper.pdf
                                                                  4. Explaining Poincaré Embeddings. A wordvector is a vector representing… | by Sri | Medium, accessed on February 13, 2026, https://medium.com/@sri33/explaining-poincar%C3%A9-embeddings-d7cb9e4a2bbf
                                                                  5. Hyperbolic Sentence Embeddings - Emergent Mind, accessed on February 13, 2026, https://www.emergentmind.com/topics/hyperbolic-sentence-embeddings
                                                                  6. Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin - CVF Open Access, accessed on February 13, 2026, https://openaccess.thecvf.com/content/WACV2024/papers/Moreira_Hyperbolic_vs_Euclidean_Embeddings_in_Few-Shot_Learning_Two_Sides_of_WACV_2024_paper.pdf
                                                                  7. Poincaré Hyperbolic Disk -- from Wolfram MathWorld, accessed on February 13, 2026, https://mathworld.wolfram.com/PoincareHyperbolicDisk.html
                                                                  8. Poincaré disk model - Wikipedia, accessed on February 13, 2026, https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model
                                                                  9. Möbius Transformation and Einstein Velocity Addition in the Hyperbolic Geometry of Bolyai and Lobachevsky - ResearchGate, accessed on February 13, 2026, https://www.researchgate.net/publication/265774396_Mobius_Transformation_and_Einstein_Velocity_Addition_in_the_Hyperbolic_Geometry_of_Bolyai_and_Lobachevsky
                                                                  10. HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean Space - arXiv, accessed on February 13, 2026, https://arxiv.org/html/2409.16897v2
                                                                  11. Hyperbolic Neural Networks - arXiv, accessed on February 13, 2026, https://arxiv.org/pdf/1805.09112
                                                                  12. On projection mappings and the gradient projection method on hyperbolic space forms - Oxford Academic, accessed on February 13, 2026, https://academic.oup.com/imajna/advance-article-pdf/doi/10.1093/imanum/draf097/65661203/draf097.pdf
                                                                  13. Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification - ACL Anthology, accessed on February 13, 2026, https://aclanthology.org/2021.eacl-main.247.pdf
                                                                  14. Regex for bank transaction parsing - Stack Overflow, accessed on February 13, 2026, https://stackoverflow.com/questions/71277325/regex-for-bank-transaction-parsing
                                                                  15. Guide to Bank Statement Fields: Expert Tips for Efficient Data Extraction - Docsumo, accessed on February 13, 2026, https://www.docsumo.com/blogs/bank-statement-extraction/fields
                                                                  16. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware, accessed on February 13, 2026, https://sol.sbc.org.br/index.php/bwaif/article/download/24949/24770
                                                                  17. Jump To Hyperspace: Comparing Euclidean and Hyperbolic Loss Functions for Hierarchical Multi-Label Text Classification - ACL Anthology, accessed on February 13, 2026, https://aclanthology.org/2025.coling-main.287.pdf
                                                                  18. Generalized Category Discovery with Large Language Models in the Loop - ACL Anthology, accessed on February 13, 2026, https://aclanthology.org/2024.findings-acl.512.pdf
                                                                  19. Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers - CVF Open Access, accessed on February 13, 2026, https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Clipped_Hyperbolic_Classifiers_Are_Super-Hyperbolic_Classifiers_CVPR_2022_paper.pdf
                                                                  20. Hyperbolic Category Discovery - arXiv, accessed on February 13, 2026, https://arxiv.org/html/2504.06120v1
                                                                  21. (PDF) Hyperbolic Category Discovery - ResearchGate, accessed on February 13, 2026, https://www.researchgate.net/publication/390601334_Hyperbolic_Category_Discovery
                                                                  22. What implementation is used for `matrix_exp`? - PyTorch Forums, accessed on February 13, 2026, https://discuss.pytorch.org/t/what-implementation-is-used-for-matrix-exp/159608
                                                                  23. TpG Geoopt: Riemannian Optimization in PyTorch - Graph Representation Learning and Beyond (GRL+), accessed on February 13, 2026, https://grlplus.github.io/papers/93.pdf
                                                                  24. Hyperbolic Optimization - arXiv, accessed on February 13, 2026, https://arxiv.org/html/2509.25206v1
                                                                  25. [2602.04932] Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery - arXiv, accessed on February 13, 2026, https://arxiv.org/abs/2602.04932
                                                                  26. Hyperbolic Representations of Source Code - Amazon Science, accessed on February 13, 2026, https://assets.amazon.science/55/d9/58097f0d41b886269b30e5c68522/hyperbolic-representations-of-source-code.pdf
                                                                  27. drewwilimitis/hyperbolic-learning: Implemented Machine Learning Algorithms in Hyperbolic Geometry (MDS, K-Means, Support vector machines, etc.) - GitHub, accessed on February 13, 2026, https://github.com/drewwilimitis/hyperbolic-learning
                                                                  28. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware Attention Layer - arXiv, accessed on February 13, 2026, https://arxiv.org/html/2312.07730v1